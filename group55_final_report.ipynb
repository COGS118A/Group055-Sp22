{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f65b7d81",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "You will design and execute a machine learning project. There are a few constraints on the nature of the allowed project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff0c844",
   "metadata": {},
   "source": [
    "- The problem addressed will not be a \"toy problem\" or \"common training students problem\" like mtcars, iris, palmer penguins etc.\n",
    "- The dataset will have >1k observations and >5 variables. I'd prefer more like >10k observations and >10 variables. A general rule is that if you have >100x more observations than variables, your solution will likely generalize a lot better. The goal of training a supervised machine learning model is to learn the underlying pattern in a dataset in order to generalize well to unseen data, so choosing a large dataset is very important.\n",
    "\n",
    "\n",
    "- The project will include a model selection and/or feature selection component where you will be looking for the best setup to maximize the performance of your ML system.\n",
    "\n",
    "\n",
    "- You will evaluate the performance of your ML system using more than one appropriate metric\n",
    "- You will be writing a report describing and discussing these accomplishments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9992689c",
   "metadata": {},
   "source": [
    "Feel free to delete this description section when you hand in your proposal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605ad0b9",
   "metadata": {},
   "source": [
    "### Peer Review\n",
    "You will all have an opportunity to look at the Project Proposals of other groups to fuel your creativity and get more ideas for how you can improve your own projects.\n",
    "Both the project proposal and project checkpoint will have peer review.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df29c969",
   "metadata": {},
   "source": [
    "# Names\n",
    "Hopefully your team is at least this good. Obviously you should replace these with your names.\n",
    "- Girish Senthil\n",
    "- Grant Liu\n",
    "- Maya Xu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bdbe35",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words. It should summarize:\n",
    "- what your goal/problem is\n",
    "- what the data used represents and how they are measured\n",
    "- what you will be doing with the data\n",
    "- how to performance/success will be measured\n",
    "\n",
    "The data shows the number of certain causes of deaths for multiple countries from 2000 to 2019. The goal of our project is to predict the amount and types of deaths in future years given the trends of the last three decades by implementing a K Nearest-Neighbors algorithm. This can be applied both globally and locally for a specific country. There are many types of deaths that we will be tracking, such as road injuries, self harm, drug use, etc. But most of them will be medically related, such as Parkinson’s, kidney disease, tuberculosis, etc. To test the accuracy of our algorithm, we will compare the results to another source that contains the data of death rates and death types for the years 2019 onwards. Success will be measured based on how closely the predictions line up with the actual reported deaths. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4b8342",
   "metadata": {},
   "source": [
    "# Background\n",
    "Fill in the background and discuss the kind of prior work that has gone on in this research area here. Use inline citation to specify which references support which statements. You can do that through HTML footnotes (demonstrated here). I used to reccommend Markdown footnotes (google is your friend) because they are simpler but recently I have had some problems with them working for me whereas HTML ones always work so far. So use the method that works for you, but do use inline citations. Here is an example of inline citation. After government genocide in the 20th century, real birds were replaced with surveillance drones designed to look just like birds[1]. Use a minimum of 2 or 3 citations, but we prefer more [2]. You need enough citations to fully explain and back up important facts. Remember you are trying to explain why someone would want to answer your question or why your hypothesis is in the form that you've stated.\n",
    "\n",
    "There are people passing away everyday due to various causes. The Global Burden of Disease is a major global study on the causes of death and disease published in the medical journal The Lance [1]. With such data collection, people in the past had already done research on the top least/most death counts in the world, and the top causes of death in different countries. Behind the different causes of death in certain areas around the world, there are also important factors increasing such causes that should be taken into consideration. According to the World Health Organization, income is one of those factors. People living in a low-income country are far more likely to die of a communicable disease than a noncommunicable disease. Diarrhoeal diseases are more significant as a cause of death in low-income countries [2]. In our project, taking all the above information into consideration, we want to see the top causes of death, analyze the potential reasons behind it, and eventually further predict the causes of death in certain areas based on the dataset we got.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594f7d00",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Clearly describe the problem that you are solving. Avoid ambiguous words. The problem described should be well defined and should have at least one ML-relevant potential solution. Additionally, describe the problem thoroughly such that it is clear that the problem is quantifiable (the problem can be expressed in mathematical or logical terms), measurable (the problem can be measured by some metric and clearly observed), and replicable (the problem can be reproduced and occurs more than once).\n",
    "\n",
    "Clearly describe the problem that you are solving. Avoid ambiguous words. The problem described should be well defined and should have at least one ML-relevant potential solution. Additionally, describe the problem thoroughly such that it is clear that the problem is quantifiable (the problem can be expressed in mathematical or logical terms), measurable (the problem can be measured by some metric and clearly observed), and replicable (the problem can be reproduced and occurs more than once).\n",
    "Death is a very tragic matter that eventually affects everyone. However, while it may seem like a random event, there are patterns that can be extrapolated from how people die. For example, death by infectious diseases are much more common in third-world countries than first-world countries. While this may seem like a simple and obvious case, there are many more trends about death hidden in the data. This is the problem we will attempt to solve. By analyzing the data we hope to better understand how death affects people around the world. This can be expressed through the data points, which include the number of deaths of a specific cause in a year for a specified country. As we went through our datasheet, we noticed that the majority of our data are deaths associated with diseases recorded in different places from 1991 to 2019. We also notice that some places lack a record of diseases cases but with sufficient execution and terrorism cases that cause death. Therefore, our research will mainly focus on the number of  deaths caused by different kinds of diseases. We will take the number of terrorism and execution as a factor to explain the lack of record. As a specific example, based on the trends of previous years, we may find out that death by cardiovascular disease is increasing in Norway. While not direct proof, this may underline a growing health crisis of unhealthy living. By measuring how many people die in the years of the dataset, we hope to predict the number of future deaths. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a0c578",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Possible datasets:\n",
    "\n",
    "https://www.kaggle.com/datasets/ivanchvez/causes-of-death-our-world-in-data?select=20222703+Causes+Of+Death+Clean+Output+V2.0.csv\n",
    "https://ghdx.healthdata.org/gbd-results-tool\n",
    "https://www.who.int/data/gho/info/gho-odata-api\n",
    "\n",
    "8200 observations, 36 variables present within the first dataset listed under possible datasets. The following datasets were used in order to create the dataset listed on kaggle. \n",
    "\n",
    "A typical observation will have the region or entity (e.g. Afghanistan) with the country code, year as well as a variable for death type with the observation for each variable being the total in that year (e.g. Deaths - Malaria = 200). A majority of the variables in this dataset are the different types of Deaths, example variables include Malaria, Self-Harm, Executions, etc.\n",
    "\n",
    "Critical variables include the Entity and the year. The other variables all represent different categories of causes of death.\n",
    "\n",
    "Certain country codes are not filled out; this may have to be iteratively corrected across the dataset for all missing variables. Deaths by execution have a high amount of missing values. Given that this dataset chronologically begins in 1990, contextually it makes sense that there are not a lot of values for executions as many regions no longer have a large number of executions. Further research can be done in order to formally be able to claim that executions were not a substantial contributor to most countries’ death counts. Terrorism followed with a large amount of missing values, so along with executions both variables can be examined with contextual background information from other datasets or articles (most probably amnesty international). \n",
    "\n",
    "All other variables do not have a significant amount of missing data (<10% missing). It would be interesting to be able to split the different causes of death by age as well as gender in order to analyze whether these demographics are significantly different between causes of death in different regions and different years.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc58626",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Make sure to describe how the solution will be tested.\n",
    "\n",
    "If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "If it is appropriate to the problem statement, describe a benchmark model[3] against which your solution will be compared.\n",
    "\n",
    "We will attempt to solve this problem by using machine learninga KNN algorithms to predict the amount of future deaths and their causes, allowing countries to better allocate resources to respond to the most pressing types of deaths occurring within their borders. We will do this by analyzing trends throughout the years 1990 to 2019 to see the rate at which certain types of deaths caused by diseases increase, decrease, or stay constant. We canwill also use this data to analyze global health trends to see what health matters are impacting our lives the most. To test our solution we will use reliable data from another source, such as the World Health Organization, to compare the reported deaths with our actual predictions. For example, we can compare our prediction for the amount of deaths by cardiovascular disease in 2020 with the actual reported amount and express our comparison as a fraction to check the accuracy of our prediction. A prediction of 80,000 deaths for a report of 100,000 deaths would be represented as 80,000/100,000 which is 80% accuracy. Alternatively we can express our accuracy using percentage error. We are choosing KNN over SVM because there may not be a clear and well defined boundary between the sets that we are testing, making KNN the better choice. For example, North America and Europe are similar in terms of wealth and demographic, meaning that the types of deaths common in those continents may be similar as well. This may hold true for other continents as well, such as Africa and South America. While we could use a kernel based SVM to combat this problem, KNN will be easier to implement, making it more ideal. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ebdfcd",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "Propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms).\n",
    "\n",
    "Given that our problem that we are trying to solve is the projected amount of deaths in each region based upon previous year’s trends, and our solution of using classification of each region and its year based upon the different death variables, there are ways we can validate our solution.\n",
    "\n",
    "The primary method in which we can evaluate our model and its predictive power is by removing certain years from the dataset after training the model in order to identify if our model is able to predict close values for each of the death variables given a region and the year that we removed. We can compare these predicted values to the actual value. With this method it is important that each region is sampled proportionally, so stratified sampling will be used in order to reduce bias and variance in our test and training subsets.\n",
    "\n",
    "In order to quantify the error of the ML model when predicting the labels (Regions or Year), we can use a simple 0-1 loss where the number of incorrect predictions is divided by the number of samples in the dataset. The reverse logic can be used to evaluate model accuracy.\n",
    "\n",
    "To boost our sample size, we can also use bootstrapping in order to increase the amount of observations that we can train our model on. For our task of classification, we can use the Leave-One-out Bootstrap technique, using between 50-200 bootstrap samples for a reliable estimate (https://arxiv.org/pdf/1811.12808.pdf).\n",
    "\n",
    "We can implement a version of this through a hold out method where we split our labeled dataset into a test and training set, with the model being used to fit for the training set and then being tested against our test set in order to identify if our model can predict and generalize well to data it has not seen. \n",
    "\n",
    "to data it has not seen. \n",
    "This is a good way to measure the success of our project because we will be able to tell whether or not our KNN algorithm is successful in predicting new data points. By seeing how well our algorithm generalizes to new data, we can make sure that our model is accurately representing the data trends as a whole. This was our original goal, and as such, these are good evaluation metrics to analyze our project with.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2359b21",
   "metadata": {},
   "source": [
    "# Results\n",
    "After careful discussions, since our dataset included a lot of columns (which means it includes rich information) we decided to try different models. In order to keep everything the best organized, we highlighted the main result down below and included our detailed code of the full model set in separated files and made sure to upload them on github.\n",
    "\n",
    "### KNN, MLP, and XGBoost\n",
    "\n",
    "Detail code: https://github.com/COGS118A/Group055-Sp22/blob/main/KNN_XGBOOST_MLP_COGS118A.ipynb\n",
    "\n",
    "KNN, MLP, and XGBoost had extremely high accuracy when modeling and predicting the data from our dataset. In particular, using a SkLearn Pipeline to transform the data into ln(x + 1) format and then using PCA to decompose certain features greatly reduced training speeds while also increasing accuracy of our models, especially in the MLP model (.9996). A stratified train-test split was used, with a test size of .25. Given how high our accuracy is across all three models (> .97 for all models), it is apparent that non-linear models performed extremely well with our data.\n",
    "\n",
    "### SVM\n",
    "\n",
    "detailed code: https://github.com/COGS118A/Group055-Sp22/blob/main/COGS118A_svm.ipynb\n",
    "\n",
    "We utilized four SVM algorithms that each had a different kernel to see which one would perform the best. The kernels we tested were “linear”, “rbf”, “poly” and “sigmoid”. Of the four, the linear kernel performed the best with an accuracy of 0.816. The rbf kernel came in second with an accuracy of 0.751, while the last two kernels were both sub 0.5 accuracy. We utilized a high C value of C=100 and a pipeline along with StandardScaler. Accuracy noticeably increased as the value of C got larger. \n",
    "After working with our models, we also noticed the uncommon high accuracy of the KNN, MLP, and the XGBoost. We will discuss it more later in the limitation part.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ca26e6",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "## Interpreting the Results\n",
    "Based off the results, it appears that the KNN/ MLP/ XGBOOST algorithms all performed extremely well in classifying the test data. The linear kernel svm was very serviceable as well, as it had a fairly high accuracy. A possible reason for the great success of the KNN algorithm is that the data is structured in a way that perfectly fits how KNN works. To explain, the data consists of multiple countries over the course of 30 years. This means that each country has 30 data points and it is highly probable that they are all close to each other in space. Thus, when testing, it is likely that the nearest neighbors of a datapoint are from that same country, which by extension, will be on the same continent (share the same label). This could perhaps explain the extremely high accuracy of KNN. It is also perhaps possible that the accuracy of the different SVM that we tested could climb even higher when utilizing an even larger C value. Although, this is untested as we decided to use C=100 as our maximum value. \n",
    "\n",
    "XGBoost and the MLP models both work well for this classification due to their respective model architectures and how they interact with our dataset with large dimensionality and 6 total labels. XGBoost is an ensemble learning technique that implements boosting, which is a sequential process that seeks to correct classification errors of previous models. Instead of assigning/updating weights in a given training model, XGBoost will fit the new model to new residuals and minimize the loss of this new training model. Given our data with extremely similar data points across rows of 30 (30 samples from each country, countries grouped into continents), the model performed well due to such high similarity in points as well as the gradient boosting across all its training models. MLP uses dropout layers instead of boosting as well as back propagation in order to update weights across the different hidden layers. At first the MLP model was unable to converge on the un-transformed data, but after using both a natural log transformation and PCA the model converged rapidly with the highest accuracy possible. A key reason as to why these nonlinear models have such high accuracy is due to the high similarity between points in the continents. For example, North America will have very similar death rates in its points due to being composed of n x 30 data points that are from the same region.\n",
    "\n",
    "\n",
    "## Limitations\n",
    "### Data cleaning part: \n",
    "In our dataset, not all the countries can be included. There were some invalid country codes that pc.coutnry_alpha2_to_continent_code() could not distinguish. Therefore we had to drop some of the rows from the dataset. While some of the data are not countries ( there are rows named the high/medium or low incomes, which are not countries for sure), there are still certain countries that are not properly associated with another specific continent. Due to the limited time we had and for future conveniences in analyzing, we eventually decided to drop those rows. If we have more time, we may hardcode the countries with their proper continents because those data are still valuable to include. \n",
    "### Data Testing Part:\n",
    "In our abstract, we were originally planning to find resources from other authorities. However, since the dataset we used included overall too many columns, including from the continent to the different reasons of death, it is very difficult for us to find suitable dataset from other places to test our proposed models. Therefore, we changed the testing strategies by splitting our clean dataset into two groups- one for training the model and one for testing the model. This actually makes our work easier because the data from the same cleaned dataset are set into the same formats so there is no additional data cleaning needed when it comes to the model testing part. We still recognize this change as a minor limitation because it differs from our original plan- finding new data sources - and it is a last-time change.\n",
    "### High accuracy:\n",
    "While we want the accuracy to be as high as possible, we do realize that the accuracy of KNN is too high (0.9996). We do have the concern that the model’s accuracy is too high. There could be a couple of reasons that caused the over high accuracy, it might be overfitting, or just because the testing dataset is not enough. As we tried the three models (KNN / MLP / XGBOOST), all of them have very high accuracy. We think the potential reason that caused the high accuracy might be not enough dataset. If we have more time, we would definitely like to explore more about what exactly causes such high accuracy.\n",
    "\n",
    "\n",
    "## Ethics & Privacy\n",
    "While there are no obvious ethical issues such as identifiable information or any such information regarding participants, there can be certain ethical issues such as representing a region or having biased data within our dataset. These issues can be investigated further in EDA where we can identify whether there are certain biases per region.\n",
    "\n",
    "As all of these variables as well as the data for each observation is queried from public data (Amnesty International, WHO, GHO), there are no issues regarding anonymity and there are no variables directly related to protected variable groups. \n",
    "\n",
    "One glaring issue that may arise from our dataset(s) as well as how we are going to use them to analytically find a solution to our proposed problem is possible biases in how regions are represented, or the type of death rates that can be problematic if they are extrapolated to years that are not within the range of our dataset. This ethical issue can be handled with validation data that we can gather independent of our data sources in order to validate our results or address possible issues within our model. \n",
    "\n",
    "Terrorism and Executions will be a variable with many ethical implications as they are polarized topics and many of the values are missing in our first dataset. This can cause misrepresentation as well as blind spots within our analysis that could affect our model ’s efficacy and interpretations. In order to address this, further research will have to be done in order to have conclusive numbers for many of the regions for deaths related to Terrorism as well as Execution. The definition for Terrorism and Executions will also have to be explored and standardized, as Terrorism could possibly extend to public shootings and other incidents of that type. Executions can also be an overarching term for capital punishments. \n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org\n",
    "\n",
    "## Conclusion\n",
    "In conclusion, it seems that most of the models we tested were highly successful in correctly classifying our test data. However, if we had to choose one, we would choose MLP or KNN because they seem to be best suited to the problem at hand. Ultimately though, our project went through many different iterations of models in order to understand what model functioned the best given our highly dimensional data. Our original goal and our proposed solution have changed a lot compared to our current project, but this is largely due to us rounding out our ideas and fully figuring out what made sense as well as what was feasibly possible to do. But, as a result, we believe that this changed our project for the better. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293ef801",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "\n",
    "[1]: Global Burden of Disease. The Lancet. https://www.thelancet.com/gbd/about\n",
    "\n",
    "[2]: World Health Organization (2020, December) The top 10 causes of death. https://www.who.int/news-room/fact-sheets/detail/the-top-10-causes-of-death\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25387acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
